# -*- coding: utf-8 -*-
"""Llama-2-7b-chat-hf-text-generation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qvKUGjhP0EBSEq_A0_OmKCnUcY61SI0I
"""

!pip install accelerate peft bitsandbytes transformers trl

#After that, we will load the necessary modules from these libraries.
import os
import torch
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    pipeline,
    logging,
)
from peft import LoraConfig
from trl import SFTTrainer

# Model from Hugging Face hub
base_model = "NousResearch/Llama-2-7b-chat-hf"

# New instruction dataset
guanaco_dataset = "mlabonne/guanaco-llama2-1k"

# Fine-tuned model
new_model = "llama-2-7b-chat-text-to-python"

from datasets import load_dataset

# Load the dataset
dataset = load_dataset("mlabonne/guanaco-llama2-1k")

# Print the dataset
print(dataset)
# Calculate the number of samples for validation (5%)
validation_percentage = 5
num_validation_samples = int(len(dataset["train"]) * validation_percentage / 100)

# Split the dataset
train_dataset = dataset["train"].select(range(num_validation_samples, len(dataset["train"])))
validation_dataset = dataset["train"].select(range(num_validation_samples))

# Print the format for each dataset
print("Training dataset:")
print(train_dataset)
print("\nValidation dataset:")
print(validation_dataset)

train_dataset['text'], validation_dataset['text']

#In our case, we create 4-bit quantization with NF4 type configuration using BitsAndBytes.

compute_dtype = getattr(torch, "float16")

quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=compute_dtype,
    bnb_4bit_use_double_quant=False,
)

#We will now load a model using 4-bit precision with the compute dtype "float16" from Hugging Face for faster training.
model = AutoModelForCausalLM.from_pretrained(
    base_model,
    quantization_config=quant_config,
    device_map={"": 0}
)
model.config.use_cache = False
model.config.pretraining_tp = 1

#Next, we will load the tokenizer from Hugginface and set padding_side to “right” to fix the issue with fp16.
tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

#Traditional fine-tuning of pre-trained language models (PLMs) requires updating all of the model's parameters, which is computationally expensive and requires massive amounts of data.

#Parameter-Efficient Fine-Tuning (PEFT) works by only updating a small subset of the model's most influential parameters, making it much more efficient.
peft_params = LoraConfig(
    lora_alpha=16,
    lora_dropout=0.1,
    r=64,
    bias="none",
    task_type="CAUSAL_LM",
)

#hyperparameters that can be used to optimize the training process:
training_params = TrainingArguments(
    output_dir="./results",
    num_train_epochs=2,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8,
    optim="paged_adamw_32bit",
    save_steps=25,
    logging_steps=25,
    learning_rate=2e-4,
    weight_decay=0.001,
    fp16=True,
    bf16=False,
    max_grad_norm=0.3,
    max_steps=-1,
    warmup_ratio=0.03,
    group_by_length=True,
    lr_scheduler_type="constant",
    report_to="tensorboard",
    evaluation_strategy="epoch"
)

#Supervised fine-tuning (SFT) is a key step in reinforcement learning from human feedback (RLHF). The TRL library from HuggingFace provides an easy-to-use API to create SFT models and train them on your dataset with just a few lines of code. It comes with tools to train language models using reinforcement learning, starting with supervised fine-tuning, then reward modeling, and finally, proximal policy optimization (PPO).

#We will provide SFT Trainer the model, dataset, Lora configuration, tokenizer, and training parameters.

trainer = SFTTrainer(
    model=model,
    train_dataset=train_dataset,
    eval_dataset=validation_dataset,
    peft_config=peft_params,
    dataset_text_field="text",
    max_seq_length=None,
    tokenizer=tokenizer,
    args=training_params,
    packing=False,
)

# Train model
trainer.train()

# Save trained model
trainer.model.save_pretrained("/content/result_trained")

trainer.model.save_pretrained("/content/result_trained")
trainer.tokenizer.save_pretrained("/content/result_trained")

from tensorboard import notebook
log_dir = "results/runs"
notebook.start("--logdir {} --port 4000".format(log_dir))

logging.set_verbosity(logging.CRITICAL)

prompt = "Who is Leonardo Da Vinci?"
pipe = pipeline(task="text-generation", model=model, tokenizer=tokenizer, max_length=200)
result = pipe(f"<s>[INST] {prompt} [/INST]")
print(result[0]['generated_text'])

config = {
    "task": "text-generation",
    "model": model,
    "tokenizer": tokenizer,
    "max_length": 250,
    "config": {
        "language": "en"
    }
}


logging.set_verbosity(logging.CRITICAL)

prompt = "Who is Leonardo Da Vinci?"
pipe = pipeline(**config)
result = pipe(f"[INST] {prompt} [/INST]")
print(result[0]['generated_text'])

validation_dataset['text']

#testing and loading model

import torch, gc
gc.collect()
torch.cuda.empty_cache()

import numpy as np
import pandas as pd
import os
from tqdm import tqdm
import bitsandbytes as bnb
import torch
import torch.nn as nn
import transformers
from datasets import Dataset
from peft import LoraConfig, PeftConfig
from trl import SFTTrainer
from transformers import (AutoModelForCausalLM,
                          AutoTokenizer,
                          BitsAndBytesConfig,
                          TrainingArguments,
                          pipeline,
                          logging)
from sklearn.metrics import (accuracy_score,
                             classification_report,
                             confusion_matrix)
from sklearn.model_selection import train_test_split

from datasets import load_dataset

# Ruta del modelo guardado en el dataset de Kaggle
from peft import LoraConfig, PeftModel

device_map = {"": 0}
PEFT_MODEL = "kr-manish/Llama-2-7b-chat-finetune-for-textGeneration"
#model_name = "NousResearch/Llama-2-7b-hf"

# Cargar la configuración del modelo
config = PeftConfig.from_pretrained(PEFT_MODEL)

# Cargar el modelo
model = AutoModelForCausalLM.from_pretrained(
    config.base_model_name_or_path,
    low_cpu_mem_usage=True,
    return_dict=True,
    #quantization_config=bnb_config,
    device_map="auto",
    #trust_remote_code=True,
    torch_dtype=torch.float16,
)

# Cargar el tokenizador
tokenizer=AutoTokenizer.from_pretrained(config.base_model_name_or_path)
tokenizer.pad_token = tokenizer.eos_token

# Cargar el modelo PEFT
load_model = PeftModel.from_pretrained(model, PEFT_MODEL)

test1 ="How to own a plane in the United States?"
prompt_test = test1
pipe_test = pipeline(task="text-generation",
                model=load_model,
                tokenizer=tokenizer,
                #max_length =20,
                max_new_tokens =25,
                temperature = 0.0,

                )
result_test = pipe_test(prompt_test)
#answer = result[0]['generated_text'].split("=")[-1]
answer_test = result_test[0]['generated_text']
answer_test

from huggingface_hub import notebook_login
notebook_login()

# Load the Drive helper and mount
from google.colab import drive
drive.mount('/content/drive')

"""#**Step 8: Push Model to Hugging Face Hub**

Our weights are merged and we reloaded the tokenizer. We can now push everything to the Hugging Face Hub to save our model.
"""

import locale
locale.getpreferredencoding = lambda: "UTF-8"

!huggingface-cli login

model.push_to_hub("kr-manish/Llama-2-7b-chat-finetune-for-textGeneration")

tokenizer.push_to_hub("kr-manish/Llama-2-7b-chat-finetune-for-textGeneration")

!cp -r /content/llama-2-fine-tuned-text-generation-result_trained /content/drive/MyDrive

!zip /content/llama-2-fine-tuned-text-generation-result_trained.zip /content/llama-2-fine-tuned-text-generation-result_trained

!zip -r /content/llama-2-text-generate-result_trained.zip /content/llama-2-fine-tuned-text-generation-result_trained

