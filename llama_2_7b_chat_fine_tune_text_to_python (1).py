# -*- coding: utf-8 -*-
"""Llama-2-7b-chat-fine-tune-text-to-python.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YgOHROJLZ6B2Nt5oQdnHNwbRqllzg4NZ
"""

!pip install accelerate peft bitsandbytes transformers trl

#In general, there are two foundational models that Mistral released: Mistral 7B v0.1 and Mistral 7B Instruct v0.1. The Mistral 7B v0.1 is the base foundation model, and the Mistral 7B Instruct v0.1 is a Mistral 7B v0.1 model that has been fine-tuned for conversation and question answering.

#We would need a CSV file containing a text column for the fine-tuning with Hugging Face AutoTrain. However, we would use a different text format for the base and instruction models during the fine-tuning.
from datasets import load_dataset
import pandas as pd

# Load the dataset
train =load_dataset("codeparrot/xlcost-text-to-code", "Python-program-level", split='train[:50%]')
#train= load_dataset("codeparrot/xlcost-text-to-code","Python-program-level", split='train[:10%]')
train = pd.DataFrame(train)
train

train.rename(columns={'text': 'instruction', 'code': 'output'}, inplace=True)
train

train['instruction'][0], train['output'][0]

train

train['instruction'].isnull().sum()
print(train[train['instruction'] == ''])

print(train[train['instruction'] == ''].reset_index(drop = True))

train_chat = train.copy()
print("1111111111111", train_chat)

##If you want to fine-tune the Mistral 7B Instruct v0.1 for conversation and question answering, we need to follow the chat template format provided by Mistral, shown in the code block below.

#<s>[INST] Instruction [/INST] Model answer</s>[INST] Follow-up instruction [/INST]


#If we use our previous example dataset, we need to reformat the text column. We would use only the data without any input for the chat model.

#Then, we could reformat the data with the following code.

def chat_formatting(data):

  text = f"<s>[INST] {data['instruction']} [/INST] {data['output']} </s>"

  return text

train_chat['text'] = train_chat.apply(chat_formatting, axis =1)
train_chat.to_csv('train_text-to-code.csv', index =False)
print("22222222222", train_chat)

train_chat['text'][0], train_chat

train_text_to_code =train_chat.copy()
train_text_to_code.to_csv('data/train_text-to-code.csv', index =False)
print("33333333", train_text_to_code)

print(train_text_to_code['text'][0])
print(len(train_text_to_code['text'][0]))

!rm -rf /content/results

#After that, we will load the necessary modules from these libraries.
import os
import torch
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    pipeline,
    logging,
)
from peft import LoraConfig
from trl import SFTTrainer

# Model from Hugging Face hub
base_model = "NousResearch/Llama-2-7b-chat-hf"

# New instruction dataset
guanaco_dataset = "mlabonne/guanaco-llama2-1k"

# Fine-tuned model
new_model = "llama-2-7b-chat-text-to-python"

#load dataset
'''
import pandas as pd
train_text_to_code =pd.read_csv("/content/data/train_text-to-code.csv")
print("111111111", train_text_to_code.head())
train_text_to_code =train_text_to_code[['text']]
dataset =train_text_to_code.copy()
dataset.head()
'''

import pandas as pd
from sklearn.model_selection import train_test_split
from datasets import Dataset, DatasetDict

# Load the CSV file
df = pd.read_csv("/content/data/train_text-to-code.csv")
#df = df.iloc[0:1500, :]
train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)

# Reset index to remove __index_level_0__
train_df.reset_index(drop=True, inplace=True)
val_df.reset_index(drop=True, inplace=True)

# Create train and validation datasets
train_data = Dataset.from_pandas(train_df[['text']])
eval_data = Dataset.from_pandas(val_df[['text']])

# Create DatasetDict
dataset_dict = DatasetDict({
    "train": train_data,
    "validation": eval_data
})

print(dataset_dict)

dataset_dict['train']['text'], dataset_dict['validation']['text']

dataset_train = dataset_dict['train']
dataset_val = dataset_dict['validation']

dataset_train

dataset_train['text']

#In our case, we create 4-bit quantization with NF4 type configuration using BitsAndBytes.

compute_dtype = getattr(torch, "float16")

quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=compute_dtype,
    bnb_4bit_use_double_quant=False,
)

#We will now load a model using 4-bit precision with the compute dtype "float16" from Hugging Face for faster training.
model = AutoModelForCausalLM.from_pretrained(
    base_model,
    quantization_config=quant_config,
    device_map={"": 0}
)
model.config.use_cache = False
model.config.pretraining_tp = 1

#Next, we will load the tokenizer from Hugginface and set padding_side to “right” to fix the issue with fp16.
tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

#Traditional fine-tuning of pre-trained language models (PLMs) requires updating all of the model's parameters, which is computationally expensive and requires massive amounts of data.

#Parameter-Efficient Fine-Tuning (PEFT) works by only updating a small subset of the model's most influential parameters, making it much more efficient.
peft_params = LoraConfig(
    lora_alpha=16,
    lora_dropout=0.1,
    r=64,
    bias="none",
    task_type="CAUSAL_LM",
)

#hyperparameters that can be used to optimize the training process:
training_params = TrainingArguments(
    output_dir="./results",
    num_train_epochs=1,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8,
    optim="paged_adamw_32bit",
    save_steps=25,
    logging_steps=25,
    learning_rate=2e-4,
    weight_decay=0.001,
    fp16= True,
    bf16=False,
    max_grad_norm=0.3,
    max_steps=-1,
    warmup_ratio=0.03,
    group_by_length=True,
    lr_scheduler_type="constant",
    report_to="tensorboard",
    evaluation_strategy="epoch"
)

#Supervised fine-tuning (SFT) is a key step in reinforcement learning from human feedback (RLHF). The TRL library from HuggingFace provides an easy-to-use API to create SFT models and train them on your dataset with just a few lines of code. It comes with tools to train language models using reinforcement learning, starting with supervised fine-tuning, then reward modeling, and finally, proximal policy optimization (PPO).

#We will provide SFT Trainer the model, dataset, Lora configuration, tokenizer, and training parameters.

trainer = SFTTrainer(
    model=model,
    train_dataset=dataset_train,
    eval_dataset=dataset_val,
    peft_config=peft_params,
    dataset_text_field="text",
    max_seq_length=None,
    tokenizer=tokenizer,
    args=training_params,
    packing=False,
)

# Train model
trainer.train()

# Save trained model
trainer.model.save_pretrained(new_model)
trainer.tokenizer.save_pretrained(new_model)



#automatically push to hub with all require things
trainer.push_to_hub("end of training")

#testing and loading model

import torch, gc
gc.collect()
torch.cuda.empty_cache()

import numpy as np
import pandas as pd
import os
from tqdm import tqdm
import bitsandbytes as bnb
import torch
import torch.nn as nn
import transformers
from datasets import Dataset
from peft import LoraConfig, PeftConfig
from trl import SFTTrainer
from transformers import (AutoModelForCausalLM,
                          AutoTokenizer,
                          BitsAndBytesConfig,
                          TrainingArguments,
                          pipeline,
                          logging)
from sklearn.metrics import (accuracy_score,
                             classification_report,
                             confusion_matrix)
from sklearn.model_selection import train_test_split

from datasets import load_dataset

#teasting----1

# Ruta del modelo guardado en el dataset de Kaggle
from peft import LoraConfig, PeftModel

device_map = {"": 0}
PEFT_MODEL = "kr-manish/Llama-2-7b-chat-fine-tune-text-to-python"
#model_name = "NousResearch/Llama-2-7b-hf"

# Cargar la configuración del modelo
config = PeftConfig.from_pretrained(PEFT_MODEL)

# Cargar el modelo
model = AutoModelForCausalLM.from_pretrained(
    config.base_model_name_or_path,
    low_cpu_mem_usage=True,
    return_dict=True,
    #quantization_config=bnb_config,
    device_map="auto",
    #trust_remote_code=True,
    torch_dtype=torch.float16,
)

# Cargar el tokenizador
tokenizer=AutoTokenizer.from_pretrained(config.base_model_name_or_path)
tokenizer.pad_token = tokenizer.eos_token

# Cargar el modelo PEFT
load_model = PeftModel.from_pretrained(model, PEFT_MODEL)

input_text ="Program to convert Centimeters to Pixels | Function to convert centimeters to pixels ; Driver Code"
prompt_test = input_text
pipe_test = pipeline(task="text-generation",
                model=load_model,
                tokenizer=tokenizer,
                max_length =200,
                #max_new_tokens =25,
                )
#result_test = pipe_test(prompt_test)
#answer_test = result_test[0]['generated_text']
#answer_test
#or
result = pipe_test(f"<s>[INST] {input_text} [/INST]")
print(result[0]['generated_text'])

dataset_val['text'][0]

