# -*- coding: utf-8 -*-
"""image_captiopn-fine-tune.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZhjZtfyWkr67vrB7ytlrRL1tF1heHLRY
"""

!pip install transformers datasets evaluate -q
!pip install jiwer -q

from huggingface_hub import notebook_login

notebook_login()

from datasets import load_dataset

#ds = load_dataset("lambdalabs/pokemon-blip-captions")
ds = load_dataset("polinaeterna/pokemon-blip-captions")

ds

ds['train'][0]

ds = ds["train"].train_test_split(test_size=0.1)
train_ds = ds["train"]
test_ds = ds["test"]

ds

from textwrap import wrap
import matplotlib.pyplot as plt
import numpy as np


def plot_images(images, captions):
    plt.figure(figsize=(20, 20))
    for i in range(len(images)):
        ax = plt.subplot(1, len(images), i + 1)
        caption = captions[i]
        caption = "\n".join(wrap(caption, 12))
        plt.title(caption)
        plt.imshow(images[i])
        plt.axis("off")


sample_images_to_visualize = [np.array(train_ds[i]["image"]) for i in range(5)]
sample_captions = [train_ds[i]["text"] for i in range(5)]
plot_images(sample_images_to_visualize, sample_captions)

#Preprocess the dataset
#Since the dataset has two modalities (image and text), the pre-processing pipeline will preprocess images and the captions.

#To do so, load the processor class associated with the model you are about to fine-tune.

from transformers import AutoProcessor

checkpoint = "microsoft/git-base"
processor = AutoProcessor.from_pretrained(checkpoint)

#The processor will internally pre-process the image (which includes resizing, and pixel scaling) and tokenize the caption.


def transforms(example_batch):
    images = [x for x in example_batch["image"]]
    captions = [x for x in example_batch["text"]]
    inputs = processor(images=images, text=captions, padding="max_length")
    inputs.update({"labels": inputs["input_ids"]})
    return inputs


train_ds.set_transform(transforms)
test_ds.set_transform(transforms)

test_ds

#With the dataset ready, you can now set up the model for fine-tuning.

#Load a base model
#Load the “microsoft/git-base” into a AutoModelForCausalLM object.


from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(checkpoint)

#Evaluate
#Image captioning models are typically evaluated with the Rouge Score or Word Error Rate. For this guide, you will use the Word Error Rate (WER).

from evaluate import load
import torch

wer = load("wer")


def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predicted = logits.argmax(-1)
    decoded_labels = processor.batch_decode(labels, skip_special_tokens=True)
    decoded_predictions = processor.batch_decode(predicted, skip_special_tokens=True)
    wer_score = wer.compute(predictions=decoded_predictions, references=decoded_labels)
    return {"wer_score": wer_score}

#Train!
#start fine-tuning the model.
#!pip install accelerate -U
from transformers import TrainingArguments, Trainer

model_name = checkpoint.split("/")[1]
"""
training_args = TrainingArguments(
    output_dir=f"{model_name}-pokemon",
    learning_rate=5e-5,
    num_train_epochs=50,
    fp16=True,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    gradient_accumulation_steps=2,
    save_total_limit=3,
    evaluation_strategy="steps",
    eval_steps=50,
    save_strategy="steps",
    save_steps=50,
    logging_steps=50,
    remove_unused_columns=False,
    push_to_hub=True,
    label_names=["labels"],
    load_best_model_at_end=True,
)
"""
training_args = TrainingArguments(
    output_dir=f"{model_name}-pokemon",
    learning_rate=3e-5,
    num_train_epochs=20,
    fp16=True,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    gradient_accumulation_steps=2,
    save_total_limit=3,
    evaluation_strategy="steps",
    eval_steps=50,
    save_strategy="steps",
    save_steps=50,
    logging_steps=50,
    remove_unused_columns=False,
    push_to_hub=True,
    label_names=["labels"],
    load_best_model_at_end=True,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_ds,
    eval_dataset=test_ds,
    compute_metrics=compute_metrics,
)

"""import torch, gc
gc.collect()
torch.cuda.empty_cache()
"""
trainer.train()

#!rm -rf /content/git-base-pokemon

#Once training is completed, share your model to the Hub with the push_to_hub() method so everyone can use your model:

trainer.push_to_hub()

#Inference
#Take a sample image from test_ds to test the model.

from PIL import Image
import requests

url = "https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/pokemon.png"
image = Image.open(requests.get(url, stream=True).raw)
image

#Prepare image for the model.

device = "cuda" if torch.cuda.is_available() else "cpu"

inputs = processor(images=image, return_tensors="pt").to(device)
pixel_values = inputs.pixel_values

#Call generate and decode the predictions.

generated_ids = model.generate(pixel_values=pixel_values, max_length=50)
generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(generated_caption)

from transformers import AutoProcessor, AutoModelForCausalLM
import torch
from PIL import Image
import requests

#Preprocess the dataset
#Since the dataset has two modalities (image and text), the pre-processing pipeline will preprocess images and the captions.

#To do so, load the processor class associated with the model you are about to fine-tune.

from transformers import AutoProcessor

checkpoint = "microsoft/git-base"
processor = AutoProcessor.from_pretrained(checkpoint)

device = "cuda" if torch.cuda.is_available() else "cpu"

model_name = "kr-manish/git-base-pokemon"  # Replace with your actual username and model name
model = AutoModelForCausalLM.from_pretrained(model_name).to(device)

url =  "https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/pokemon.png"  # Replace with the URL of your image
image = Image.open(requests.get(url, stream=True).raw)
inputs = processor(images=image, return_tensors="pt").to(device)

generated_ids = model.generate(pixel_values=inputs.pixel_values, max_length=50)
generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(generated_caption)

